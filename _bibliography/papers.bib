---
---

@inproceedings{gehrke2018invisible,
  title={The invisible maze task (IMT): interactive exploration of sparse virtual environments to investigate action-driven formation of spatial representations},
  author={Gehrke, Lukas and Iversen, John R and Makeig, Scott and Gramann, Klaus},
  booktitle={Spatial Cognition XI: 11th International Conference, Spatial Cognition 2018, T{\"u}bingen, Germany, September 5-8, 2018, Proceedings 11},
  pages={293--310},
  year={2018},
  organization={Springer International Publishing},
  doi={10.1007/978-3-319-96385-3_20},
  preview={invisible-maze-task.png},
  website={https://link.springer.com/chapter/10.1007/978-3-319-96385-3_20},
  abstract={The neuroscientific study of human navigation has been constrained by the prerequisite of traditional brain imaging studies that require participants to remain stationary. Such imaging approaches neglect a central component that characterizes navigation - the multisensory experience of self-movement. Navigation by active movement through space combines multisensory perception with internally generated self-motion cues. We investigated the spatial microgenesis during free ambulatory exploration of interactive sparse virtual environments using motion capture synchronized to high resolution electroencephalographic (EEG) data as well AS psychometric and self-report measures. In such environments, map-like allocentric representations must be constructed out of transient, egocentric first-person perspective 3-D spatial information. Considering individual differences of spatial learning ability, we studied if changes in exploration behavior coincide with spatial learning of an environment. To this end, we analyzed the quality of sketch maps (a description of spatial learning) that were produced after repeated learning trials for differently complex maze environments. We observed significant changes in active exploration behavior from the first to the last exploration of a maze: a decrease in time spent in the maze predicted an increase in subsequent sketch map quality. Furthermore, individual differences in spatial abilities as well as differences in the level of experienced immersion had an impact on the quality of spatial learning. Our results demonstrate converging evidence of observable behavioral changes associated with spatial learning in a framework that allows the study of cortical dynamics of navigation.},
  selected={false},
  dimensions={true}
}

@article{jungnickel2019mobi,
  title={MoBI—Mobile brain/body imaging},
  author={Jungnickel, Evelyn and Gehrke, Lukas and Klug, Marius and Gramann, Klaus},
  journal={Neuroergonomics},
  pages={59--63},
  year={2019},
  publisher={Academic Press},
  doi={10.1016/B978-0-12-811926-6.00010-5},
  website={https://www.sciencedirect.com/science/article/pii/B9780128119266000105},
  abstract={Mobile brain/body imaging (MoBI) is an integrative multimethod approach used to investigate human brain activity, motor behavior, and other physiological data associated with cognitive processes that involve active behavior. This chapter reviews the basic principles behind MoBI, recording instrumentation, and best practice of different processing and analyses approaches. The focus is on electroencephalography as the only portable method to image the human brain with sufficient temporal resolution to investigate fine-grained subsecond-scale cognitive processes. The chapter shows how controlled and modifiable experimental environments can be used to investigate natural cognition and active behavior in a wide range of applications in neuroergonomics and beyond.},
  selected={false},
  dimensions={true}
}

@inproceedings{gehrke2019extracting,
  title={Extracting Motion-Related Subspaces from EEG in Mobile Brain/Body Imaging Studies using Source Power Comodulation},
  author={Gehrke, Lukas and Guerdan, Luke and Gramann, Klaus},
  booktitle={2019 9th International IEEE/EMBS Conference on Neural Engineering (NER)},
  pages={344--347},
  year={2019},
  organization={IEEE},
  doi={10.1109/NER.2019.8717157},
  preview={spoc.png},
  website={https://ieeexplore.ieee.org/abstract/document/8717157},
  abstract={Mobile Brain/Body Imaging (MoBI) is an emerging non-invasive approach to investigate human brain activity and motor behavior associated with cognitive processes in natural conditions. MoBI studies and analyses pipelines combine brain measurements, e.g. Electroencephalography (EEG), with motion data as participants conduct tasks with near-natural behavior. Within the field however, standard source decomposition and reconstruction pipelines largely rely on unsupervised blind source separation (BSS) approaches and do not consider movement information to guide the decomposition of oscillatory brain sources. We propose the use of a supervised spatial filtering method, Source Power Co-modulation (SPoC), for extracting source components that co-modulate with body motion. Further, we introduce a method to validate the quality of oscillatory sources in MoBI studies. We illustrate the approach to investigate the link between hand and head movement kinematics and power dynamics of EEG sources while participants explore an invisible maze in virtual reality. Stable oscillatory source envelopes correlating with hand and head motion were isolated in all subjects, with median ρ = .13 for all sources and median ρ = .16 for sources passing the selection criteria. The results indicate that it is possible to improve movement related source separation to further guide our understanding of how movement and brain dynamics interact.},
  selected={false},
  dimensions={true}
}

@inproceedings{gehrke2019detecting,
  title={Detecting visuo-haptic mismatches in virtual reality using the prediction error negativity of event-related brain potentials},
  author={Gehrke, Lukas and Akman, Sezen and Lopes, Pedro and Chen, Albert and Singh, Avinash Kumar and Chen, Hsiang-Ting and Lin, Chin-Teng and Gramann, Klaus},
  booktitle={Proceedings of the 2019 CHI conference on human factors in computing systems},
  pages={1--11},
  year={2019},
  doi={10.1145/3290605.3300657},
  preview={prediction-error.png},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3290605.3300657},
  website={https://dl.acm.org/doi/abs/10.1145/3290605.3300657},
  abstract={Designing immersion is the key challenge in virtual reality; this challenge has driven advancements in displays, rendering and recently, haptics. To increase our sense of physical immersion, for instance, vibrotactile gloves render the sense of touching, while electrical muscle stimulation (EMS) renders forces. Unfortunately, the established metric to assess the effectiveness of haptic devices relies on the user's subjective interpretation of unspecific, yet standardized, questions.
Here, we explore a new approach to detect a conflict in visuo-haptic integration (e.g., inadequate haptic feedback based on poorly configured collision detection) using electroencephalography (EEG). We propose analyzing event-related potentials (ERPs) during interaction with virtual objects. In our study, participants touched virtual objects in three conditions and received either no haptic feedback, vibration, or vibration and EMS feedback. To provoke a brain response in unrealistic VR interaction, we also presented the feedback prematurely in 25% of the trials.
We found that the early negativity component of the ERP (so called prediction error) was more pronounced in the mismatch trials, indicating we successfully detected haptic conflicts using our technique. Our results are a first step towards using ERPs to automatically detect visuo-haptic mismatches in VR, such as those that can cause a loss of the user's immersion.},
  selected={true},
  dimensions={true}
}

@article{scott2019neurofeedback,
  title={Neurofeedback during creative expression as a therapeutic tool},
  author={Scott, Stephanie M and Gehrke, Lukas},
  journal={Mobile brain-body imaging and the neuroscience of art, innovation and creativity},
  pages={161--166},
  year={2019},
  publisher={Springer International Publishing},
  doi={10.1007/978-3-030-24326-5_17},
  preview={art-brain-computer-interface-lukas-gehrke.png},
  website={https://link.springer.com/chapter/10.1007/978-3-030-24326-5_17},
  abstract={Engaging users within therapeutic and rehabilitative trainings is a challenge towards sparking, and maintaining motivation. We explored how electroencephalographic (EEG) signals may be used to engage patients, and promote creative rehabilitation and therapeutic interventions. We introduce a proof-of-concept measuring EEG during therapeutic drawing to adapt an interactive canvas online.},
  selected={false},
  dimensions={true}
}

@article{delaux2021mobile,
  title={Mobile brain/body imaging of landmark-based navigation with high-density EEG},
  author={Delaux, Alexandre and de Saint Aubert, Jean-Baptiste and Ramano{\"e}l, Stephen and B{\'e}cu, Marcia and Gehrke, Lukas and Klug, Marius and Chavarriaga, Ricardo and Sahel, Jos{\'e}-Alain and Gramann, Klaus and Arleo, Angelo},
  journal={European Journal of Neuroscience},
  volume={54},
  number={12},
  pages={8256--8282},
  year={2021},
  doi={10.1111/ejn.15190},
  preview={landmark-based-navigation.png},
  website={https://onlinelibrary.wiley.com/doi/full/10.1111/ejn.15190},
  abstract={Coupling behavioral measures and brain imaging in naturalistic, ecological conditions is key to comprehend the neural bases of spatial navigation. This highly integrative function encompasses sensorimotor, cognitive, and executive processes that jointly mediate active exploration and spatial learning. However, most neuroimaging approaches in humans are based on static, motion-constrained paradigms and they do not account for all these processes, in particular multisensory integration. Following the Mobile Brain/Body Imaging approach, we aimed to explore the cortical correlates of landmark-based navigation in actively behaving young adults, solving a Y-maze task in immersive virtual reality. EEG analysis identified a set of brain areas matching state-of-the-art brain imaging literature of landmark-based navigation. Spatial behavior in mobile conditions additionally involved sensorimotor areas related to motor execution and proprioception usually overlooked in static fMRI paradigms. Expectedly, we located a cortical source in or near the posterior cingulate, in line with the engagement of the retrosplenial complex in spatial reorientation. Consistent with its role in visuo-spatial processing and coding, we observed an alpha-power desynchronization while participants gathered visual information. We also hypothesized behavior-dependent modulations of the cortical signal during navigation. Despite finding few differences between the encoding and retrieval phases of the task, we identified transient time–frequency patterns attributed, for instance, to attentional demand, as reflected in the alpha/gamma range, or memory workload in the delta/theta range. We confirmed that combining mobile high-density EEG and biometric measures can help unravel the brain structures and the neural modulations subtending ecological landmark-based navigation.},
  selected={false},
  dimensions={true}
}

@article{miyakoshi2021audiomaze,
  title={The AudioMaze: An EEG and motion capture study of human spatial navigation in sparse augmented reality},
  author={Miyakoshi, Makoto and Gehrke, Lukas and Gramann, Klaus and Makeig, Scott and Iversen, John},
  journal={European Journal of Neuroscience},
  volume={54},
  number={12},
  pages={8283--8307},
  year={2021},
  doi={10.1111/ejn.15131},
  preview={audiomaze.jpg},
  website={https://onlinelibrary.wiley.com/doi/full/10.1111/ejn.15131},
  abstract={Spatial navigation is one of the fundamental cognitive functions central to survival in most animals. Studies in humans investigating the neural foundations of spatial navigation traditionally use stationary, desk-top protocols revealing the hippocampus, parahippocampal place area (PPA), and retrosplenial complex to be involved in navigation. However, brain dynamics, while freely navigating the real world remain poorly understood. To address this issue, we developed a novel paradigm, the AudioMaze, in which participants freely explore a room-sized virtual maze, while EEG is recorded synchronized to motion capture. Participants (n = 16) were blindfolded and explored different mazes, each in three successive trials, using their right hand as a probe to “feel” for virtual maze walls. When their hand “neared” a virtual wall, they received directional noise feedback. Evidence for spatial learning include shortening of time spent and an increase of movement velocity as the same maze was repeatedly explored. Theta-band EEG power in or near the right lingual gyrus, the posterior portion of the PPA, decreased across trials, potentially reflecting the spatial learning. Effective connectivity analysis revealed directed information flow from the lingual gyrus to the midcingulate cortex, which may indicate an updating process that integrates spatial information with future action. To conclude, we found behavioral evidence of navigational learning in a sparse-AR environment, and a neural correlate of navigational learning was found near the lingual gyrus.},
  selected={false},
  dimensions={true}
}

@article{gehrke2021single,
  title={Single-trial regression of spatial exploration behavior indicates posterior EEG alpha modulation to reflect egocentric coding},
  author={Gehrke, Lukas and Gramann, Klaus},
  journal={European Journal of Neuroscience},
  volume={54},
  number={12},
  pages={8318--8335},
  year={2021},
  doi={10.1111/ejn.15152},
  preview={invisible-maze-task-eeg-results-teaser.png},
  website={https://onlinelibrary.wiley.com/doi/full/10.1111/ejn.15152},
  abstract={Learning to navigate uncharted terrain is a key cognitive ability that emerges as a deeply embodied process, with eye movements and locomotion proving most useful to sample the environment. We studied healthy human participants during active spatial learning of room-scale virtual reality (VR) mazes. In the invisible maze task, participants wearing a wireless electroencephalography (EEG) headset were free to explore their surroundings, only given the objective to build and foster a mental spatial representation of their environment. Spatial uncertainty was resolved by touching otherwise invisible walls that were briefly rendered visible inside VR, similar to finding your way in the dark. We showcase the capabilities of mobile brain/body imaging using VR, demonstrating several analysis approaches based on general linear models (GLMs) to reveal behavior-dependent brain dynamics. Confirming spatial learning via drawn sketch maps, we employed motion capture to image spatial exploration behavior describing a shift from initial exploration to subsequent exploitation of the mental representation. Using independent component analysis, the current work specifically targeted oscillations in response to wall touches reflecting isolated spatial learning events arising in deep posterior EEG sources located in the retrosplenial complex. Single-trial regression identified significant modulation of alpha oscillations by the immediate, egocentric, exploration behavior. When encountering novel walls, as well as with increasing walking distance between subsequent touches when encountering novel walls, alpha power decreased. We conclude that these oscillations play a prominent role during egocentric evidencing of allocentric spatial hypotheses.},
  selected={false},
  dimensions={true}  
}

@article{gramann2021human,
  title={Human cortical dynamics during full-body heading changes},
  author={Gramann, Klaus and Hohlefeld, Friederike U and Gehrke, Lukas and Klug, Marius},
  journal={Scientific Reports},
  volume={11},
  number={1},
  pages={18186},
  year={2021},
  publisher={Nature Publishing Group UK London},
  doi={10.1038/s41598-021-97749-8},
  preview={spot-rotation.png},
  website={https://www.nature.com/articles/s41598-021-97749-8},
  abstract={The retrosplenial complex (RSC) plays a crucial role in spatial orientation by computing heading direction and translating between distinct spatial reference frames based on multi-sensory information. While invasive studies allow investigating heading computation in moving animals, established non-invasive analyses of human brain dynamics are restricted to stationary setups. To investigate the role of the RSC in heading computation of actively moving humans, we used a Mobile Brain/Body Imaging approach synchronizing electroencephalography with motion capture and virtual reality. Data from physically rotating participants were contrasted with rotations based only on visual flow. During physical rotation, varying rotation velocities were accompanied by pronounced wide frequency band synchronization in RSC, the parietal and occipital cortices. In contrast, the visual flow rotation condition was associated with pronounced alpha band desynchronization, replicating previous findings in desktop navigation studies, and notably absent during physical rotation. These results suggest an involvement of the human RSC in heading computation based on visual, vestibular, and proprioceptive input and implicate revisiting traditional findings of alpha desynchronization in areas of the navigation network during spatial orientation in movement-restricted participants.},
  selected={false},
  dimensions={true}  
}

@article{gehrke2022neural,
  title={Neural sources of prediction errors detect unrealistic VR interactions},
  author={Gehrke, Lukas and Lopes, Pedro and Klug, Marius and Akman, Sezen and Gramann, Klaus},
  journal={Journal of Neural Engineering},
  volume={19},
  number={3},
  pages={036002},
  year={2022},
  publisher={IOP Publishing},
  doi={10.1088/1741-2552/ac69bc},
  preview={pen-jne.png},
  pdf={https://lukasgehrke.com/paper_pdfs/2021-JournalofNeuralEngineering.pdf},
  website={https://iopscience.iop.org/article/10.1088/1741-2552/ac69bc/meta},
  abstract={Objective. Neural interfaces hold significant promise to implicitly track user experience. Their application in virtual and augmented reality (VR/AR) simulations is especially favorable as it allows user assessment without breaking the immersive experience. In VR, designing immersion is one key challenge. Subjective questionnaires are the established metrics to assess the effectiveness of immersive VR simulations. However, administering such questionnaires requires breaking the immersive experience they are supposed to assess. Approach. We present a complimentary metric based on a event-related potentials. For the metric to be robust, the neural signal employed must be reliable. Hence, it is beneficial to target the neural signal's cortical origin directly, efficiently separating signal from noise. To test this new complementary metric, we designed a reach-to-tap paradigm in VR to probe electroencephalography (EEG) and movement adaptation to visuo-haptic glitches. Our working hypothesis was, that these glitches, or violations of the predicted action outcome, may indicate a disrupted user experience. Main results. Using prediction error negativity features, we classified VR glitches with 77% accuracy. We localized the EEG sources driving the classification and found midline cingulate EEG sources and a distributed network of parieto-occipital EEG sources to enable the classification success. Significance. Prediction error signatures from these sources reflect violations of user's predictions during interaction with AR/VR, promising a robust and targeted marker for adaptive user interfaces.},
  selected={true},
  video={https://www.youtube.com/watch?v=EDayGdlEqsU},
  dimensions={true}  
}

@incollection{gehrke2022toward,
  title={Toward Human Augmentation Using Neural Fingerprints of Affordances},
  author={Gehrke, Lukas and Lopes, Pedro and Gramann, Klaus},
  booktitle={Affordances in Everyday Life: A Multidisciplinary Collection of Essays},
  pages={173--180},
  year={2022},
  publisher={Springer International Publishing Cham},
  doi={10.1007/978-3-031-08629-8_16},
  preview={affordance.png},
  pdf={https://link.springer.com/content/pdf/10.1007/978-3-031-08629-8.pdf},
  website={https://link.springer.com/chapter/10.1007/978-3-031-08629-8_16},
  abstract={In our increasingly complex world, many objects in our environment do not readily afford their intended use case. From a designer’s viewpoint, the challenge then is to design for easily perceived utility. In this essay, we discuss a system to implement affordances on the user directly. We present the idea to leverage brain activity and actuation hardware to design the emergence of affordances. Our proposed system (1) tracks a user’s readiness for action in near real time through the electroencephalogram (EEG) and (2) implements affordances by cueing and physically moving the user’s body using haptic devices that can directly actuate the user’s muscles, such as motor-based exoskeletons or electrical muscle stimulation.},
  selected={false},
  video={https://www.youtube.com/watch?v=j6CLBU0j34c},
  dimensions={true}  
}

@article{klug2022bemobil,
  title={The BeMoBIL Pipeline for automated analyses of multimodal mobile brain and body imaging data},
  author={Klug, Marius and Jeung, Sein and Wunderlich, Anna and Gehrke, Lukas and Protzak, Janna and Djebbara, Zakaria and Argubi-Wollesen, Andreas and Wollesen, Bettina and Gramann, Klaus},
  journal={bioRxiv},
  pages={2022--09},
  year={2022},
  publisher={Cold Spring Harbor Laboratory},
  doi={10.1101/2022.09.29.510051},
  preview={bemobil-pipeline.png},
  pdf={https://www.biorxiv.org/content/10.1101/2022.09.29.510051v2.full.pdf},
  website={https://www.biorxiv.org/content/10.1101/2022.09.29.510051v2.abstract},
  abstract={Advancements in hardware technology and analysis methods allow more and more mobility in electroencephalography (EEG) experiments. Mobile Brain/Body Imaging (MoBI) studies may record various types of data such as motion or eye tracking in addition to neural activity. Although there are options available to analyze EEG data in a standardized way, they do not fully cover complex multimodal data from mobile experiments. We thus propose the BeMoBIL Pipeline, an easy-to-use pipeline in MATLAB that supports the time-synchronized handling of multimodal data. It is based on EEGLAB and fieldtrip and consists of automated functions for EEG preprocessing and subsequent source separation. It also provides functions for motion data processing and extraction of event markers from different data modalities, including the extraction of eye-movement and gait-related events from EEG using independent component analysis. The pipeline introduces a new robust method for region-of-interest-based group-level clustering of independent EEG components. Finally, the BeMoBIL Pipeline provides analytical visualizations at various processing steps, keeping the analysis transparent and allowing for quality checks of the resulting outcomes. All parameters and steps are documented within the data structure and can be fully replicated using the same scripts. This pipeline makes the processing and analysis of (mobile) EEG and body data more reliable and independent of the prior experience of the individual researchers, thus facilitating the use of EEG in general and MoBI in particular. It is an open-source project available for download at https://github.com/BeMoBIL/bemobil-pipeline which allows for community-driven adaptations in the future.},
  selected={false},
  dimensions={true}  
}

@incollection{jeung2022virtual,
  title={Virtual reality for spatial navigation},
  author={Jeung, Sein and Hilton, Christopher and Berg, Timotheus and Gehrke, Lukas and Gramann, Klaus},
  year={2022},
  doi={10.1007/7854_2022_403},
  preview={vr_navigation.png},
  pdf={https://link.springer.com/content/pdf/10.1007/978-3-031-42995-8.pdf},
  website={https://link.springer.com/chapter/10.1007/7854_2022_403},
  abstract={Immersive virtual reality (VR) allows its users to experience physical space in a non-physical world. It has developed into a powerful research tool to investigate the neural basis of human spatial navigation as an embodied experience. The task of wayfinding can be carried out by using a wide range of strategies, leading to the recruitment of various sensory modalities and brain areas in real-life scenarios. While traditional desktop-based VR setups primarily focus on vision-based navigation, immersive VR setups, especially mobile variants, can efficiently account for motor processes that constitute locomotion in the physical world, such as head-turning and walking. When used in combination with mobile neuroimaging methods, immersive VR affords a natural mode of locomotion and high immersion in experimental settings, designing an embodied spatial experience. This in turn facilitates ecologically valid investigation of the neural underpinnings of spatial navigation.},
  selected={false},
  dimensions={true}  
}

@inproceedings{pangratz2023towards,
  title={Towards an Implicit Metric of Sensory-Motor Accuracy: Brain Responses to Auditory Prediction Errors in Pianists},
  author={Pangratz, Elisabeth and Chiossi, Francesco and Villa, Steeven and Gramann, Klaus and Gehrke, Lukas},
  booktitle={Proceedings of the 15th Conference on Creativity and Cognition},
  pages={129--138},
  year={2023},
  doi={10.1145/3591196.3593340},
  preview={cc23-piano.png},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3591196.3593340},
  website={https://dl.acm.org/doi/abs/10.1145/3591196.3593340},
  abstract={During listening to music, the brain expects specific acoustic events based on learned musical rules. During music performance expectancy is additionally created based on motor action by linking keypresses to their sounds. We investigated EEG (Electroencephalography) signals to auditory expectancy violations in piano performance and perception. In our study, pianists experienced manipulations of different acoustic features, such as pitch and loudness, during playing and listening to piano sequences. We found that manipulations during performance elicited deflections with stronger amplitudes compared to manipulations during perception indicating that the action of producing sounds strengthens auditory expectancy. Loudness manipulations, violating musical regularity, elicited deflections with smaller latencies compared to pitch manipulations, which violate harmonic expectancy, suggesting that the brain processes expectancy violations of distinct acoustic features in a different way. These EEG signatures may prove useful for applications in intelligent music interfaces by providing information about sensory-motor accuracy.},
  selected={false},
  video={https://www.youtube.com/watch?v=j6CLBU0j34c},
  dimensions={true}
}

@article{nguyen2023modeling,
  title={Modeling the Intent to Interact with VR using Physiological Features},
  author={Nguyen, Willy and Gramann, Klaus and Gehrke, Lukas},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2023},
  publisher={IEEE},
  doi={10.1109/TVCG.2023.3308787},
  preview={2023-TVCG.png},
  pdf={https://lukasgehrke.com/paper_pdfs/2021-JournalofNeuralEngineering.pdf},
  website={https://ieeexplore.ieee.org/abstract/document/10230891},
  abstract={Abstract— Objective. Mixed-Reality (XR) technologies promise a user experience (UX) that rivals the interactive experience with the real-world. The key facilitators in the design of such a natural UX are that the interaction has zero lag and that users experience no excess mental load. This is difficult to achieve due to technical constraints such as motion-to-photon latency as well as false-positives during gesture-based interaction.
Methods. In this paper, we explored the use of physiological features to model the user’s intent to interact with a virtual reality (VR) environment. Accurate predictions about when users want to express an interaction intent could overcome the limitations of an interactive device that lags behind the intention of a user. We computed time-domain features from electroencephalography (EEG) and electromyography (EMG) recordings during a grab-and-drop task in VR and cross-validated a Linear Discriminant Analysis (LDA) for three different combinations of (1) EEG, (2) EMG and (3) EEG-EMG features.
Results & Conclusion. We found the classifiers to detect the presence of a pre-movement state from background idle activity reflecting the users’ intent to interact with the virtual objects (EEG: 62% ± 10%, EMG: 72% ± 9%, EEG-EMG: 69% ± 10%) above simulated chance level. The features leveraged in our classification scheme have a low computational cost and are especially useful for fast decoding of users’ mental states. Our work is a further step towards a useful classification of users’ intent to interact, as a high temporal resolution and speed of detection is crucial. This facilitates natural experiences through zero-lag adaptive interfaces.},
  selected={true},
  dimensions={true}  
}

@article{10.1145/3660345,
  author = {Terfurth, Leonie and Gramann, Klaus and Gehrke, Lukas},
  title = {Decoding Realism of Virtual Objects: Exploring Behavioral and Ocular Reactions to Inaccurate Interaction Feedback},
  year = {2024},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {1073-0516},
  url = {https://doi.org/10.1145/3660345},
  doi = {10.1145/3660345},
  abstract = {Achieving temporal synchrony between sensory modalities is crucial for natural perception of object interaction in virtual reality. While subjective questionnaires are currently used to evaluate users’ VR experiences, leveraging behavior and psychophysiological responses can provide additional insights.We investigated motion and ocular behavior as discriminators between realistic and unrealistic object interactions. Participants grasped and placed a virtual object while experiencing sensory feedback that either matched their expectations or occurred too early. We also explored visual-only feedback vs. combined visual and haptic feedback. Due to technological limitations, a condition with delayed feedback was added post-hoc.Gaze-based metrics revealed discrimination between high and low feedback realism. Increased interaction uncertainty was associated with longer fixations on the avatar hand and temporal shifts in the gaze-action relationship. Our findings enable real-time evaluation of users’ perception of realism in interactions. They facilitate the optimization of interaction realism in virtual environments and beyond.},
  note = {Just Accepted},
  journal = {ACM Trans. Comput.-Hum. Interact.},
  month = {apr},
  keywords = {virtual reality, object interaction, interaction realism, gaze behavior, user experience},
  selected={true},
  dimensions={true},
  pdf={https://lukasgehrke.com/paper_pdfs/10.1145_3660345.pdf},
  website={https://dl.acm.org/doi/10.1145/3660345},
  preview={2024-tochi.png}
}