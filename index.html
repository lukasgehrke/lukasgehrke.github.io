<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> </title> <meta name="author" content=" "> <meta name="description" content="Lukas Gehrke's personal website featuring his research at the intersection of cognitive neuroscience and human-computer interaction. "> <meta name="keywords" content="research, EEG, HCI, jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%89&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lukasgehrke.github.io/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">home <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">workshops </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/presence-workshop-berlin-2024/">Measuring Presence in XR</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/bids-workshop-neuroergonomics-2024/">Workflows for sharing multimodal recordings in BIDS format</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lukas-480.webp 480w,/assets/img/lukas-800.webp 800w,/assets/img/lukas-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/lukas.jpg?3093130de76f72409fc2802b463b01f3" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="lukas.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <a href="https://scholar.google.de/citations?user=I7s0gkMAAAAJ&amp;hl=en" class="ai ai-google-scholar ai-2x" style="margin-right: 10px;" rel="external nofollow noopener" target="_blank"></a> <a href="https://linkedin.com/in/lukasgehrke/" class="fa-brands fa-linkedin fa-2x" style="margin-right: 10px;" rel="external nofollow noopener" target="_blank"></a> <a href="mailto:info@lukasgehrke.com" class="fa-solid fa-envelope fa-2x"></a> <br><br> <h2 style="color: inherit">Lukas <strong>Gehrke</strong> </h2> <p>PhD Candidate</p> <br><br> <p><a href="https://www.tu.berlin" rel="external nofollow noopener" target="_blank">TU Berlin</a></p> <p><a href="https://www.tu.berlin/bpn" rel="external nofollow noopener" target="_blank">Department of Biopsychology and Neuroergonomics</a></p> </div> </div> <div class="clearfix"> <p><br><br></p> <p>I work at the intersection of human-computer interaction (HCI) and cognitive neuroscience. In my PhD research, I published papers on multimodal interface technology, investigating neural- and movement signatures for novel natural interaction experiences with extended realities (XR).</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Oct 16, 2024</th> <td> With <a href="https://martinfeick.com" rel="external nofollow noopener" target="_blank">Martin Feick</a> at UIST 2024 in Pittsburgh. Martin presented our paper <a href="https://dl.acm.org/doi/abs/10.1145/3654777.3676425" rel="external nofollow noopener" target="_blank">“Predicting the Limits: Tailoring Unnoticeable Hand Redirection Offsets in Virtual Reality to Individuals’ Perceptual Boundaries”</a>. See a brief demo video here: <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div class="embed-responsive embed-responsive-16by9"> <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/TwDDXDbuu1o" allowfullscreen=""></iframe> </div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/10-24_uist-pittsburgh/demo-480.webp 480w,/assets/img/10-24_uist-pittsburgh/demo-800.webp 800w,/assets/img/10-24_uist-pittsburgh/demo-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/10-24_uist-pittsburgh/demo.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 27, 2024</th> <td> Invited to <a href="https://vrs.ruhr-uni-bochum.de" rel="external nofollow noopener" target="_blank">VR Summit Bochum</a> and helping out with the hackathon. Was a fun event with a great lineup of speakers. <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9-24_vrs-bochum/lukas1-480.webp 480w,/assets/img/9-24_vrs-bochum/lukas1-800.webp 800w,/assets/img/9-24_vrs-bochum/lukas1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/9-24_vrs-bochum/lukas1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9-24_vrs-bochum/lukas2-480.webp 480w,/assets/img/9-24_vrs-bochum/lukas2-800.webp 800w,/assets/img/9-24_vrs-bochum/lukas2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/9-24_vrs-bochum/lukas2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 24, 2024</th> <td> Intense three days of discussions about presence experience in VR and how, if, it can be measured. We were super lucky to have the <a href="https://www.digital-future.berlin" rel="external nofollow noopener" target="_blank">Einstein Center for Digital Future</a> host our meeting. Planning to write up a report of the meeting and the findings of the discussions. <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7-24_berlin-presence/group-480.webp 480w,/assets/img/7-24_berlin-presence/group-800.webp 800w,/assets/img/7-24_berlin-presence/group-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/7-24_berlin-presence/group.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7-24_berlin-presence/einstein-480.webp 480w,/assets/img/7-24_berlin-presence/einstein-800.webp 800w,/assets/img/7-24_berlin-presence/einstein-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/7-24_berlin-presence/einstein.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 12, 2024</th> <td> Neuroergonomics conference in Bordeaux was good fun and came with a surprise. I was awarded the <a href="https://neuroergonomics2024.inria.fr/awards/" rel="external nofollow noopener" target="_blank">Raja Parasuraman: Young Investigator Award sponsored by BIONIC</a> at <a href="http://neuroergonomics2024.inria.fr" rel="external nofollow noopener" target="_blank">Neuroergonomics 2024</a> conference for the paper “Agency-preserving action augmentation: towards preemptive muscle control using brain-computer interfaces“. The paper, which I wrote with Leonie Terfurth and Klaus Gramann is on <a href="https://arxiv.org/abs/2409.16896" rel="external nofollow noopener" target="_blank">arXiv</a> now! <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7-24_nec-bordeaux/talk-480.webp 480w,/assets/img/7-24_nec-bordeaux/talk-800.webp 800w,/assets/img/7-24_nec-bordeaux/talk-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/7-24_nec-bordeaux/talk.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </td> </tr> <tr> <th scope="row" style="width: 20%">May 06, 2024</th> <td> I now published the data we reported on in two earlier two publications at <a href="https://dl.acm.org/doi/abs/10.1145/3290605.3300657" rel="external nofollow noopener" target="_blank">CHI</a> and in <a href="https://iopscience.iop.org/article/10.1088/1741-2552/ac69bc/meta" rel="external nofollow noopener" target="_blank">JNE</a>. The data is available on <a href="https://openneuro.org/datasets/ds003846/versions/2.0.2" rel="external nofollow noopener" target="_blank">openneuro</a> and the detailed specification can now be found in <a href="https://www.frontiersin.org/journals/neuroergonomics/articles/10.3389/fnrgo.2024.1411305/full" rel="external nofollow noopener" target="_blank">Frontiers in Neuroergonomics</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">May 03, 2024</th> <td> Happy to announce a workshop on defining outstanding challenges in measuring the experience of presence in XR. More information can be found <a href="https://lukasgehrke.notion.site/Measuring-Presence-in-XR-State-of-the-art-and-Outstanding-Challenges-fe4858e113844f3b8798c5d68e39a55f?pvs=4" rel="external nofollow noopener" target="_blank">here</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 10, 2024</th> <td> Workshop on BIDS at <a href="http://neuroergonomics2024.inria.fr" rel="external nofollow noopener" target="_blank">Neuroergonomics 2024</a>: Together with Sein Jeung, we will be teaching a workshop on the BIDS (Brain Imaging Data Standard) format. More information can be found <a href="/bids-workshop-neuroergonomics-2024/">here</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 20, 2024</th> <td> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/3-24_ieeevr-orlando/talk-480.webp 480w,/assets/img/3-24_ieeevr-orlando/talk-800.webp 800w,/assets/img/3-24_ieeevr-orlando/talk-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/3-24_ieeevr-orlando/talk.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/3-24_ieeevr-orlando/firework-480.webp 480w,/assets/img/3-24_ieeevr-orlando/firework-800.webp 800w,/assets/img/3-24_ieeevr-orlando/firework-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/3-24_ieeevr-orlando/firework.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> Two talks at IEEE VR ‘24 in Disneyland Orlando. First, I presented a <a href="https://ieeexplore.ieee.org/abstract/document/10536309" rel="external nofollow noopener" target="_blank">workshop paper</a>. The paper presents a regression analyses of presence experience scores on motion behavior in a large-scale walking VR task. During the main conference I presented a TVCG paper written with Willy Nguyen, a previuos student, on <a href="https://ieeexplore.ieee.org/abstract/document/10230891" rel="external nofollow noopener" target="_blank">modeling user’s intent to interact using physiological features</a>, in our case EEG. <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/3-24_ieeevr-orlando/vr-demo1-480.webp 480w,/assets/img/3-24_ieeevr-orlando/vr-demo1-800.webp 800w,/assets/img/3-24_ieeevr-orlando/vr-demo1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/3-24_ieeevr-orlando/vr-demo1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/3-24_ieeevr-orlando/vr-demo2-480.webp 480w,/assets/img/3-24_ieeevr-orlando/vr-demo2-800.webp 800w,/assets/img/3-24_ieeevr-orlando/vr-demo2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/3-24_ieeevr-orlando/vr-demo2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Tried out some VR interfaces in the demo session, was really impressed by some smell and heat rendering interfaces. </div> </td> </tr> </table> </div> </div> </article> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024-arxiv_agency-480.webp 480w,/assets/img/publication_preview/2024-arxiv_agency-800.webp 800w,/assets/img/publication_preview/2024-arxiv_agency-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/2024-arxiv_agency.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-arxiv_agency.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="gehrke2024senseagencyclosedloopmuscle" class="col-sm-8"> <div class="title">Sense of Agency in Closed-loop Muscle Stimulation</div> <div class="author"> Lukas Gehrke, Leonie Terfurth, and Klaus Gramann </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="https://lukasgehrke.github.io/agency-ems/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2024-tochi-480.webp 480w,/assets/img/publication_preview/2024-tochi-800.webp 800w,/assets/img/publication_preview/2024-tochi-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/2024-tochi.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2024-tochi.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="10.1145/3660345" class="col-sm-8"> <div class="title">Decoding Realism of Virtual Objects: Exploring Behavioral and Ocular Reactions to Inaccurate Interaction Feedback</div> <div class="author"> Leonie Terfurth, Klaus Gramann, and Lukas Gehrke </div> <div class="periodical"> <em>ACM Trans. Comput.-Hum. Interact.</em>, Apr 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://lukasgehrke.com/paper_pdfs/10.1145_3660345.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://dl.acm.org/doi/10.1145/3660345" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1145/3660345" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Achieving temporal synchrony between sensory modalities is crucial for natural perception of object interaction in virtual reality. While subjective questionnaires are currently used to evaluate users’ VR experiences, leveraging behavior and psychophysiological responses can provide additional insights.We investigated motion and ocular behavior as discriminators between realistic and unrealistic object interactions. Participants grasped and placed a virtual object while experiencing sensory feedback that either matched their expectations or occurred too early. We also explored visual-only feedback vs. combined visual and haptic feedback. Due to technological limitations, a condition with delayed feedback was added post-hoc.Gaze-based metrics revealed discrimination between high and low feedback realism. Increased interaction uncertainty was associated with longer fixations on the avatar hand and temporal shifts in the gaze-action relationship. Our findings enable real-time evaluation of users’ perception of realism in interactions. They facilitate the optimization of interaction realism in virtual environments and beyond.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2023-TVCG-480.webp 480w,/assets/img/publication_preview/2023-TVCG-800.webp 800w,/assets/img/publication_preview/2023-TVCG-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/2023-TVCG.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2023-TVCG.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="nguyen2023modeling" class="col-sm-8"> <div class="title">Modeling the Intent to Interact with VR using Physiological Features</div> <div class="author"> Willy Nguyen, Klaus Gramann, and Lukas Gehrke </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://lukasgehrke.com/paper_pdfs/2021-JournalofNeuralEngineering.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://ieeexplore.ieee.org/abstract/document/10230891" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/TVCG.2023.3308787" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Abstract— Objective. Mixed-Reality (XR) technologies promise a user experience (UX) that rivals the interactive experience with the real-world. The key facilitators in the design of such a natural UX are that the interaction has zero lag and that users experience no excess mental load. This is difficult to achieve due to technical constraints such as motion-to-photon latency as well as false-positives during gesture-based interaction. Methods. In this paper, we explored the use of physiological features to model the user’s intent to interact with a virtual reality (VR) environment. Accurate predictions about when users want to express an interaction intent could overcome the limitations of an interactive device that lags behind the intention of a user. We computed time-domain features from electroencephalography (EEG) and electromyography (EMG) recordings during a grab-and-drop task in VR and cross-validated a Linear Discriminant Analysis (LDA) for three different combinations of (1) EEG, (2) EMG and (3) EEG-EMG features. Results &amp; Conclusion. We found the classifiers to detect the presence of a pre-movement state from background idle activity reflecting the users’ intent to interact with the virtual objects (EEG: 62% ± 10%, EMG: 72% ± 9%, EEG-EMG: 69% ± 10%) above simulated chance level. The features leveraged in our classification scheme have a low computational cost and are especially useful for fast decoding of users’ mental states. Our work is a further step towards a useful classification of users’ intent to interact, as a high temporal resolution and speed of detection is crucial. This facilitates natural experiences through zero-lag adaptive interfaces.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/pen-jne-480.webp 480w,/assets/img/publication_preview/pen-jne-800.webp 800w,/assets/img/publication_preview/pen-jne-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/pen-jne.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="pen-jne.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="gehrke2022neural" class="col-sm-8"> <div class="title">Neural sources of prediction errors detect unrealistic VR interactions</div> <div class="author"> Lukas Gehrke, Pedro Lopes, Marius Klug, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Sezen Akman, Klaus Gramann' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Journal of Neural Engineering</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://lukasgehrke.com/paper_pdfs/2021-JournalofNeuralEngineering.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=EDayGdlEqsU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://iopscience.iop.org/article/10.1088/1741-2552/ac69bc/meta" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1088/1741-2552/ac69bc" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Objective. Neural interfaces hold significant promise to implicitly track user experience. Their application in virtual and augmented reality (VR/AR) simulations is especially favorable as it allows user assessment without breaking the immersive experience. In VR, designing immersion is one key challenge. Subjective questionnaires are the established metrics to assess the effectiveness of immersive VR simulations. However, administering such questionnaires requires breaking the immersive experience they are supposed to assess. Approach. We present a complimentary metric based on a event-related potentials. For the metric to be robust, the neural signal employed must be reliable. Hence, it is beneficial to target the neural signal’s cortical origin directly, efficiently separating signal from noise. To test this new complementary metric, we designed a reach-to-tap paradigm in VR to probe electroencephalography (EEG) and movement adaptation to visuo-haptic glitches. Our working hypothesis was, that these glitches, or violations of the predicted action outcome, may indicate a disrupted user experience. Main results. Using prediction error negativity features, we classified VR glitches with 77% accuracy. We localized the EEG sources driving the classification and found midline cingulate EEG sources and a distributed network of parieto-occipital EEG sources to enable the classification success. Significance. Prediction error signatures from these sources reflect violations of user’s predictions during interaction with AR/VR, promising a robust and targeted marker for adaptive user interfaces.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/prediction-error-480.webp 480w,/assets/img/publication_preview/prediction-error-800.webp 800w,/assets/img/publication_preview/prediction-error-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/prediction-error.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="prediction-error.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="gehrke2019detecting" class="col-sm-8"> <div class="title">Detecting visuo-haptic mismatches in virtual reality using the prediction error negativity of event-related brain potentials</div> <div class="author"> Lukas Gehrke, Sezen Akman, Pedro Lopes, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Albert Chen, Avinash Kumar Singh, Hsiang-Ting Chen, Chin-Teng Lin, Klaus Gramann' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 2019 CHI conference on human factors in computing systems</em> , Apr 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://dl.acm.org/doi/pdf/10.1145/3290605.3300657" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://dl.acm.org/doi/abs/10.1145/3290605.3300657" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1145/3290605.3300657" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Designing immersion is the key challenge in virtual reality; this challenge has driven advancements in displays, rendering and recently, haptics. To increase our sense of physical immersion, for instance, vibrotactile gloves render the sense of touching, while electrical muscle stimulation (EMS) renders forces. Unfortunately, the established metric to assess the effectiveness of haptic devices relies on the user’s subjective interpretation of unspecific, yet standardized, questions. Here, we explore a new approach to detect a conflict in visuo-haptic integration (e.g., inadequate haptic feedback based on poorly configured collision detection) using electroencephalography (EEG). We propose analyzing event-related potentials (ERPs) during interaction with virtual objects. In our study, participants touched virtual objects in three conditions and received either no haptic feedback, vibration, or vibration and EMS feedback. To provoke a brain response in unrealistic VR interaction, we also presented the feedback prematurely in 25% of the trials. We found that the early negativity component of the ERP (so called prediction error) was more pronounced in the mismatch trials, indicating we successfully detected haptic conflicts using our technique. Our results are a first step towards using ERPs to automatically detect visuo-haptic mismatches in VR, such as those that can cause a loss of the user’s immersion.</p> </div> </div> </div> </li> </ol> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. <a href="https://lukasgehrke.github.io/impressum/">Impressum</a>. Last updated: February 17, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>